{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# This is a language analysis notebook.\n",
        "We will be comparing articles from Bulbapedia and a few other selected texts to English, French and German.\\"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dfcf81e7",
      "metadata": {},
      "source": [
        "## Proposed similarity function\n",
        "\n",
        "Below is an implementation of a proposed language confidence function that compares a text a dictionary of most common words in a language.  \n",
        "For each word in the text, let $p$ be its frequency in the text and $q$ its frequency in the dictionary, both being values in range $[0, 1]$.  \n",
        "That word's score is defined as $(p-q)^2$ if the word was found in the language dictionary. If not, the score is instead equal to $max(p^2, (1-p)^2)$, being the supremum of $(p-q)^2$ for all possible values of $q$.  \n",
        "Each word's score denotes a degree of misalignment between the text and the language. The average of all word scores $A$ can be interpreted as the entire text's misallignment. Naturally, the confidence (similarity) score will be equal to $1-A$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def lang_confidence_score(word_counts: dict[str, float], language_words_with_frequency: dict[str, float]) -> float:\n",
        "    if(len(word_counts) == 0):\n",
        "        return 1\n",
        "    number_of_words = 0\n",
        "    for val in word_counts.items():\n",
        "        number_of_words += val[1]\n",
        "    deviation = 0\n",
        "    for word in word_counts:\n",
        "        if(word not in word_counts or word not in language_words_with_frequency): \n",
        "            # Dictionary miss - we impose the most severe penalty.\n",
        "            deviation += max(word_counts[word] / number_of_words, 1 - word_counts[word] / number_of_words)**2\n",
        "        else:\n",
        "            # Word found in dictionary - we impose penalty based on difference of frequency in text and frequency in dictionary.\n",
        "            deviation += (word_counts[word] / number_of_words - language_words_with_frequency[word])**2\n",
        "    return 1 - deviation / len(word_counts)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "481d27b5",
      "metadata": {},
      "source": [
        "We use the wordfreq package to select 1000 most frequent words in each language."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import wordfreq \n",
        "\n",
        "def create_lang_dict(size, language):\n",
        "    words = wordfreq.top_n_list(language, size, wordlist='best')\n",
        "    d = {w: wordfreq.word_frequency(w, language) for w in set(words)}\n",
        "    return d\n",
        "\n",
        "\n",
        "en_dict = create_lang_dict(1000, 'en')\n",
        "fr_dict = create_lang_dict(1000, 'fr')\n",
        "de_dict = create_lang_dict(1000, 'de')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Large Bulbapedia article (Ash Ketchum)\n",
        "We will use the article about the Pokemon Series main character Ash Ketchum as a representative."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "p = Path('./word-counts.json')\n",
        "\n",
        "if(p.exists()):\n",
        "    p.unlink()\n",
        "\n",
        "%run wikiscraper.py --count-words Ash\n",
        "\n",
        "with open('./word-counts.json', 'r') as file:\n",
        "    large_article = json.load(file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Interesting article\n",
        "We wish to highlight some differences between English and Bulbapedia. An article with a list of Pokemon names was chosen after a selection process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "p = Path('./word-counts.json')\n",
        "\n",
        "lowest_score = 1\n",
        "\n",
        "if(p.exists()):\n",
        "    p.unlink()\n",
        "\n",
        "%run wikiscraper.py --count-words 'List of Pok√©mon by weight'\n",
        "\n",
        "with open('./word-counts.json', 'r') as file:\n",
        "    interesting_article = json.load(file) \n",
        "        \n",
        "# Verify that the score is low\n",
        "\n",
        "print(lang_confidence_score(interesting_article, en_dict))\n",
        "print(lang_confidence_score(interesting_article, fr_dict))\n",
        "print(lang_confidence_score(interesting_article, de_dict))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Classic literature\n",
        "We chose \"Frankenstein\", \"Faust\" and \"Candide\" as texts representative of English, German and French."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def text_to_dict(book_path):\n",
        "    with open(book_path, 'r') as file:\n",
        "        text_content = file.read()\n",
        "    text_content = ''.join(list(map(lambda c: c.lower() if (c.isalpha() or c == ' ') else ' ', text_content)))\n",
        "    \n",
        "    counter: dict[str, int] = {}\n",
        "    for word in text_content.split():\n",
        "        counter[word] = counter.get(word, 0) + 1\n",
        "    return counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "frankenstein_dict = text_to_dict('./frankenstein.txt')\n",
        "faust_dict = text_to_dict('./faust.txt')\n",
        "candide_dict = text_to_dict('./candide.txt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Comparison chart\n",
        "Below is a chart comparing our texts to each language's $k$ most frequent words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d66be170",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "texts = [large_article, interesting_article, frankenstein_dict, faust_dict, candide_dict]\n",
        "labels = ['Large Article', 'Interesting Article', 'Frankenstein', 'Faust', 'Candide']\n",
        "languages = ['en', 'fr', 'de']\n",
        "\n",
        "fig, axes = plt.subplots(2, 2)\n",
        "axes = axes.flatten()\n",
        "\n",
        "for idx, k in enumerate([3, 10, 100, 1000]):\n",
        "    matrix = np.empty([len(texts), len(languages)])\n",
        "    for i, t in enumerate(texts):\n",
        "        for j, l in enumerate(languages):\n",
        "            matrix[i][j] = lang_confidence_score(t, create_lang_dict(k, l))\n",
        "\n",
        "    x = np.arange(len(texts))\n",
        "    width = 0.2\n",
        "    ax = axes[idx]\n",
        "        \n",
        "    for j, l in enumerate(languages):\n",
        "        ax.bar(x + j*width, matrix[:, j], width, label=l)\n",
        "        \n",
        "    ax.set_xticks(x + width)\n",
        "    ax.set_xticklabels(labels, rotation = 30, ha='right')\n",
        "    ax.set_title(f'k = {k}')\n",
        "    ax.legend()\n",
        "\n",
        "fig.suptitle('Confidence Score')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Results\n",
        "## Effectiveness of our function\n",
        "We can see that the proposed function is somewhat accurate in identifying the language of the wiki.\n",
        "For all considered $k$'s texts score significantly higher with the dictionary of the language they are written in.\n",
        "The scored assigned by our function should be monotonic with respect to $k$ (the larger the $k$, the less dictionary misses).\n",
        "This is clearly confirmed by our examples, as all scores increase significantly with large $k$'s.\n",
        "Even though the assigned scores are relatively low, the function succeeds at effectively identyfing the language of our text - even for $k$ as low as 10.\n",
        "## Specifics of considered languages\n",
        "All three language dictionaries perform similarly despite the confidence function relying on the inflection of the words to correctly identify matches. \n",
        "However, German and Faust slightly lag behind with its assigned scores - this is possibly due to declation.\n",
        "Despite the above, the chart alone does not provide enough information to unambigiously state which language has the most word inflection - the results are simply too similar.\n",
        "## Pokemon lingo vs English language\n",
        "Finding an article with a relatively low English score was difficult. Scraping random articles proved to be ineffective, so my best guess was an article with a list of Pokemons.\n",
        "Surprisingly, even though it largely consists of Pokemon names (which of course are not in the English dictionary), its score for $k=1000$ is only around 2 times smaller than\n",
        "the flagship article about Ash Ketchum, which consists of long paragraphs written in (mostly) conventional English.  "
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
